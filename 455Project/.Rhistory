constVariance = function(losers)
{
ggplot(dataset,aes(x=losers$fitted.values,y=losers$residuals))+geom_point()
}
isNormal = function(model)
{
qqnorm(residuals(model),ylab="residuals",main="")
qqline(residuals(model))
}
seeLeverages = function(model,dataset)
{
hats = hatvalues(model)
states = row.names(dataset) #may not be needed
x = halfnorm(hats,labs=states,ylab="leverage")
}
examineStudRes = function(model)
{
studRes = rstudent(model)
studRes[which.max(abs(studRes))]
sort(studRes,decreasing = FALSE)
}
getCooks = function(model,dataset)
{
states = row.names(dataset) #may not be needed
satCook = cooks.distance(model)
halfnorm(satCook,labs=states,ylab="Cook's distance")
return(satCook)
}
#BEGIN teengamb
data(teengamb)
tmodel <- lm(gamble~.,teengamb)
constVariance(tmodel)
#this looks suspicious, based on the info given by our textbook, looks like some heteroscedasticity
# we may need to transform our y, so that var(h(y)) is constant, book used square root for counts, we can expirment with this.
constVariance(teengamb,sqrt(teengamb$gamble))
constVariance(teengamb,exp(teengamb$gamble))
constVariance(teengamb,abs(teengamb$gamble))
#none of these really worked
isNormal(tmodel)
#nope, this aint good, looks like a long tailed distribution which means we may need to use robust methods or let central limit therorem take care of this problem
#could also be an outlier, we definetly have one.
seeLeverages(tmodel,teengamb)
#ony lone of concern is unit or person # 24, row 24. They have a decently high leverage.
examineStudRes(tmodel)
#row six has a very very large studentized residual, while row 24 does not. may need to look at row 5
weirdo = getCooks(tmodel,teengamb)
#row6 has shown up again, suspicious stuff
#looking at the data, utah has a very low expend value and very high score, and a very low taker score
tmodel2 = lm(gamble~.,subset = weirdo< max(weirdo),teengamb)
summary(tmodel)
summary(tmodel2)
termplot(stateLM,partial.resid = TRUE)
library("faraway", lib.loc="~/R/win-library/3.4")
require(faraway)
data(sat)
y=sat$total
stateLM = lm(y~ expend+salary+ratio+takers,sat)
ggplot(sat,aes(x=stateLM$fitted.values,y=stateLM$residuals))+geom_point()
#not much to say, looks pretty good to me
qqnorm(residuals(stateLM),ylab="residuals",main="")
qqline(residuals(stateLM))
#still looks good so far, nothing too crazy, it MIGHT be a short tailed distribution, but this isnt too concerning due to the ultimatey normal principal
hats = hatvalues(stateLM)
states = row.names(sat)
x = halfnorm(hats,labs=states,ylab="leverage")
#Utah and California seem to have something going on. high leverage, > 2 * 7 / 50, and are beyond 2 in the half normal quantiles
studRes = rstudent(stateLM)
studRes[which.max(abs(studRes))]
sort(studRes,decreasing = FALSE)
#west virginia, utah,northDakota,NewHampshire, all have over 2 studentized residuals, west virginia is above 3!
#studentized residuals
satCook = cooks.distance(stateLM)
halfnorm(satCook,labs=states,ylab="Cook's distance")
#utah has shown up again, very large cooks distance this time around.
#looking at the data, utah has a very low expend value and very high score, and a very low taker score
stateLM2 = lm(total~expend+salary+ratio+takers,subset = satCook < max(satCook),sat)
summary(stateLM)
summary(stateLM2)
termplot(stateLM,partial.resid = TRUE)
constVariance = function(losers)
{
ggplot(dataset,aes(x=losers$fitted.values,y=losers$residuals))+geom_point()
}
isNormal = function(model)
{
qqnorm(residuals(model),ylab="residuals",main="")
qqline(residuals(model))
}
seeLeverages = function(model,dataset)
{
hats = hatvalues(model)
states = row.names(dataset) #may not be needed
x = halfnorm(hats,labs=states,ylab="leverage")
}
examineStudRes = function(model)
{
studRes = rstudent(model)
studRes[which.max(abs(studRes))]
sort(studRes,decreasing = FALSE)
}
getCooks = function(model,dataset)
{
states = row.names(dataset) #may not be needed
satCook = cooks.distance(model)
halfnorm(satCook,labs=states,ylab="Cook's distance")
return(satCook)
}
#BEGIN teengamb
data(teengamb)
tmodel <- lm(gamble~.,teengamb)
constVariance(tmodel)
#this looks suspicious, based on the info given by our textbook, looks like some heteroscedasticity
# we may need to transform our y, so that var(h(y)) is constant, book used square root for counts, we can expirment with this.
constVariance(lm(log(gamble)~.,teengamb))
constVariance(lm(exp(gamble)~.,teengamb))
constVariance(lm(sqrt(gamble)~.,teengamb))
#none of these really worked
isNormal(tmodel)
#nope, this aint good, looks like a long tailed distribution which means we may need to use robust methods or let central limit therorem take care of this problem
#could also be an outlier, we definetly have one.
seeLeverages(tmodel,teengamb)
#ony lone of concern is unit or person # 24, row 24. They have a decently high leverage.
examineStudRes(tmodel)
#row six has a very very large studentized residual, while row 24 does not. may need to look at row 5
weirdo = getCooks(tmodel,teengamb)
#row6 has shown up again, suspicious stuff
#looking at the data, utah has a very low expend value and very high score, and a very low taker score
tmodel2 = lm(gamble~.,subset = weirdo< max(weirdo),teengamb)
summary(tmodel)
summary(tmodel2)
termplot(stateLM,partial.resid = TRUE)
{
ggplot(aes(x=losers$fitted.values,y=losers$residuals))+geom_point()
}
constVariance = function(losers,dataset)
{
ggplot(dataset,aes(x=losers$fitted.values,y=losers$residuals))+geom_point()
}
constVariance(tmodel,teengamb)
tmodel <- lm(gamble~.,teengamb)
constVariance(tmodel,teengamb)
#this looks suspicious, based on the info given by our textbook, looks like some heteroscedasticity
# we may need to transform our y, so that var(h(y)) is constant, book used square root for counts, we can expirment with this.
constVariance(lm(log(gamble)~.,teengamb),teengamb)
#this looks suspicious, based on the info given by our textbook, looks like some heteroscedasticity
# we may need to transform our y, so that var(h(y)) is constant, book used square root for counts, we can expirment with this.
#constVariance(lm(log(gamble)~.,teengamb),teengamb)
constVariance(lm(exp(gamble)~.,teengamb),teengamb)
constVariance(lm(sqrt(gamble)~.,teengamb),teengamb)
tmodel <- lm(gamble~.,teengamb)
constVariance(tmodel,teengamb)
tmodel
summary(tmodel)
#this looks suspicious, based on the info given by our textbook, looks like some heteroscedasticity
# we may need to transform our y, so that var(h(y)) is constant, book used square root for counts, we can expirment with this.
#constVariance(lm(log(gamble)~.,teengamb),teengamb)
constVariance(lm(exp(gamble)~.,teengamb),teengamb)
constVariance(lm(sqrt(gamble)~.,teengamb),teengamb)
library(ggplot2)
library("faraway", lib.loc="~/R/win-library/3.4")
require(faraway)
data(sat)
y=sat$total
stateLM = lm(y~ expend+salary+ratio+takers,sat)
ggplot(sat,aes(x=stateLM$fitted.values,y=stateLM$residuals))+geom_point()
#not much to say, looks pretty good to me
qqnorm(residuals(stateLM),ylab="residuals",main="")
qqline(residuals(stateLM))
#still looks good so far, nothing too crazy, it MIGHT be a short tailed distribution, but this isnt too concerning due to the ultimatey normal principal
hats = hatvalues(stateLM)
states = row.names(sat)
x = halfnorm(hats,labs=states,ylab="leverage")
#Utah and California seem to have something going on. high leverage, > 2 * 7 / 50, and are beyond 2 in the half normal quantiles
studRes = rstudent(stateLM)
studRes[which.max(abs(studRes))]
sort(studRes,decreasing = FALSE)
#west virginia, utah,northDakota,NewHampshire, all have over 2 studentized residuals, west virginia is above 3!
#studentized residuals
satCook = cooks.distance(stateLM)
halfnorm(satCook,labs=states,ylab="Cook's distance")
#utah has shown up again, very large cooks distance this time around.
#looking at the data, utah has a very low expend value and very high score, and a very low taker score
stateLM2 = lm(total~expend+salary+ratio+takers,subset = satCook < max(satCook),sat)
summary(stateLM)
summary(stateLM2)
termplot(stateLM,partial.resid = TRUE)
constVariance = function(losers,dataset)
{
ggplot(dataset,aes(x=losers$fitted.values,y=losers$residuals))+geom_point()
}
isNormal = function(model)
{
qqnorm(residuals(model),ylab="residuals",main="")
qqline(residuals(model))
}
seeLeverages = function(model,dataset)
{
hats = hatvalues(model)
states = row.names(dataset) #may not be needed
x = halfnorm(hats,labs=states,ylab="leverage")
}
examineStudRes = function(model)
{
studRes = rstudent(model)
studRes[which.max(abs(studRes))]
sort(studRes,decreasing = FALSE)
}
getCooks = function(model,dataset)
{
states = row.names(dataset) #may not be needed
satCook = cooks.distance(model)
halfnorm(satCook,labs=states,ylab="Cook's distance")
return(satCook)
}
#BEGIN teengamb
data(teengamb)
tmodel <- lm(gamble~.,teengamb)
constVariance(tmodel,teengamb)
#this looks suspicious, based on the info given by our textbook, looks like some heteroscedasticity
# we may need to transform our y, so that var(h(y)) is constant, book used square root for counts, we can expirment with this.
#constVariance(lm(log(gamble)~.,teengamb),teengamb)
constVariance(lm(exp(gamble)~.,teengamb),teengamb)
constVariance(lm(sqrt(gamble)~.,teengamb),teengamb)
#none of these really worked
isNormal(tmodel)
#nope, this aint good, looks like a long tailed distribution which means we may need to use robust methods or let central limit therorem take care of this problem
#could also be an outlier, we definetly have one.
seeLeverages(tmodel,teengamb)
#ony lone of concern is unit or person # 24, row 24. They have a decently high leverage.
examineStudRes(tmodel)
#row six has a very very large studentized residual, while row 24 does not. may need to look at row 5
weirdo = getCooks(tmodel,teengamb)
#row6 has shown up again, suspicious stuff
#looking at the data, utah has a very low expend value and very high score, and a very low taker score
tmodel2 = lm(gamble~.,subset = weirdo< max(weirdo),teengamb)
summary(tmodel)
summary(tmodel2)
termplot(stateLM,partial.resid = TRUE)
termplot(tmodel,partial.resid = TRUE)
termplot(stateLM,partial.resid = TRUE)
#not sure what to do with this, doesnt make too much sense.
#BEGIN prostate
data(prostate)
pmodel = lm(lpsa~.,prostate)
constVariance(pmodel,prostate)
#we looking prime son
isNormal(pModel)
#we looking prime son
isNormal(pmodel)
#looks good still, may be short tailed.
seeLeverages(pmodel,prostate)
2*9/97
#observations 41 and 32 seem to
examineStudRes(pmodel)
# potential points of interested, 39, 47, 95, 69
cooksP = getCooks(pmodel,prostate)
tail(cooksP)
#graphically, 32 and 47 are quite high, along with a couple more strugglers
summary(pmodel)
pmodel2=lm(lpsa~.,subset=cooksP<max(cooksP),prostate)
summary(pmodel2)
# we see some improvement, maybe do this again but with more removed from cooks data set
#BEGIN: swiss data
data(swiss)
smodel=lm(Fertility~.swiss)
smodel=lm(fertility~.swiss)
# we see some improvement, maybe do this again but with more removed from cooks data set
#BEGIN: swiss data
data(swiss)
swiss
smodel=lm(fertility~.swiss)
smodel=lm(Fertility~.swiss)
smodel=lm(Fertility~.,swiss)
constVariance(smodel,swiss)
#looks good
isNormal(smodel)
#very very nice
seeLeverages(smodel,swiss)
2*6/47
#v de gee and la vallee are quite high
examineStudRes(smodel)
# not much here
cooksS = getCooks(smodel,swiss)
#porren and sierre are decently high
smodel2 = lm(Fertility~.,subset=cooksS<max(cooksS),swiss)
summary(smodel)
summary(smodel2)
# severly improed R squared, its tempting to always do this?
#Begin CHEDDAR
data(cheddar)
cmodel = lm(taste~.,cheddar)
constVariance(cmodel,cheddar)
#looks good here too
isNormal(cmodel)
#looks good
seeLeverages(cmodel,cheddar)
2*4/30
?gala
data(gala)
x = gala
View(gala)
GM = lm(Species~.,data=gala
)
gm2 = lm(Species~Elevation+Nearest+Scruz+Area,data=gala)
gm3=lm(Species~Elevation+Nearest+Scruz+I(Area+Adjacent))
gm3=lm(Species~Elevation+Nearest+Scruz+I(Area+Adjacent),data=gala)
anova(GM,gm2)
anova(gm2,GM)
anova(gm3,GM)
GM = lm(Species~Elevation+Nearest+Scruz+Area+Adjacent,data=gala)
anova(gm2,GM)
anova(gm3,GM)
?deviance
y = data(gala)
names(y)
y.names
y
col.names(y)
names
?names
names(gala)
names(gala)
model1=lm(Species~Area+Elevation+Nearest)
model1=lm(Species~Area+Elevation+Nearest,gala)
model2=lm(Species~Area+Elevation+Scruz,gala)
anova(model1,model2)
model3=lm(Species~Area+Elevation+Nearest,gala)
anova(model1,model3)
anova(model1,model2)
model3=lm(Species~Area+Elevation+log(Nearest),gala)
anova(model1,model3)
model4=lm(Species~Area+Elevation,gala)
anova(model1,model4)
anova(model4,model1)
summary(model1)
sumary(model1)
sumary(model4)
anova(model4,model1)
tmep = sqrt(169947)
temp/27
temp = sqrt(169947)
temp/27
temp/26
169947/27
sqsrt(6294)
sqrt(6294)
169947/26
sqrt(6536.423)
169947/27
sqrt(6294.333)
data(prostate)
pmodel
sumary(pmodel)
prostate
sumary(pmodel)
x=paste(getwd(),"/Github/BiasedEstimators/455Project",sep="")
setwd(x)
library("ggplot2", lib.loc="~/R/win-library/3.4")
library("plyr", lib.loc="~/R/win-library/3.4")
require(ggplot2)
mydata=read.csv("RegularSeasonDetailedResults.csv")
losingData= read.csv("LosingDataDetailedRegularSeason.csv")
winningData = read.csv("WinningDataDetailedRegularSeason.csv")
teamInfo = read.csv("Teams.csv")
OGseeds = read.csv("TourneySeeds.csv")
seeds = OGseeds
seeds['Team'] = teamInfo[ match(seeds[['Team']],teamInfo[['Team_Id']]),'Team_Name']
tourneyCResults = read.csv("TourneyCompactResults.csv")
regCResults = read.csv("RegularSeasonCompactResults.csv")
twn = tourneyCResults
twn['Wteam'] = teamInfo[ match(twn[['Wteam']],teamInfo[['Team_Id']]),'Team_Name']
twn['Lteam'] = teamInfo[ match(twn[['Lteam']],teamInfo[['Team_Id']]),'Team_Name']
unqTt = unique(twn$Wteam)
other = unique(twn$Lteam)
unqTt = intersect(unqTt,other)
idk = lm(Daynum~Wteam+Lteam,tourneyCResults)
twn
y1985 = twn[twn$Season == 1985,]
years = unique(twn$Season)
years
wTeamEntries = unique(y1985$Wteam)
wTeamEntries
length(wTeamEntries)
summary(idk)
OGseeds
newSeeds = OGseeds
names(OGseeds)
newSeeds['Team'] = teamInfo[ match(newSeeds[['Team']],teamInfo[['Team_Id']]),'Team_Name']
newSeeds
uniqueTeamy = unique(newSeeds[newSeeds$Season==1985,])
uniqueTeamy
getYears = function(allData,years,season)
{
for(index in 1:length(years))
{
yData = allData[allData$Season == years[index],]
teams = season[season$Season==years[index],]$Team
for(val in 1:length(yData))
{
}
}
}
uniqueTeamy = unique(newSeeds[newSeeds$Season==1985,])
getYears(twn,years,newSeeds)
yData
maxNum = numeric()
names(twn)
x=paste(getwd(),"/Github/BiasedEstimators/455Project",sep="")
setwd(x)
library("ggplot2", lib.loc="~/R/win-library/3.4")
library("plyr", lib.loc="~/R/win-library/3.4")
require(ggplot2)
mydata=read.csv("RegularSeasonDetailedResults.csv")
losingData= read.csv("LosingDataDetailedRegularSeason.csv")
winningData = read.csv("WinningDataDetailedRegularSeason.csv")
teamInfo = read.csv("Teams.csv")
OGseeds = read.csv("TourneySeeds.csv")
newSeeds = OGseeds
seeds = OGseeds
seeds['Team'] = teamInfo[ match(seeds[['Team']],teamInfo[['Team_Id']]),'Team_Name']
tourneyCResults = read.csv("TourneyCompactResults.csv")
regCResults = read.csv("RegularSeasonCompactResults.csv")
twn = tourneyCResults
twn['Wteam'] = teamInfo[ match(twn[['Wteam']],teamInfo[['Team_Id']]),'Team_Name']
twn['Lteam'] = teamInfo[ match(twn[['Lteam']],teamInfo[['Team_Id']]),'Team_Name']
newSeeds['Team'] = teamInfo[ match(newSeeds[['Team']],teamInfo[['Team_Id']]),'Team_Name']
unqTt = unique(twn$Wteam)
other = unique(twn$Lteam)
unqTt = intersect(unqTt,other)
idk = lm(Daynum~Wteam+Lteam,tourneyCResults)
savePlot = function(name,myPlot)
{
pdf(name)
print(myPlot)
dev.off()
}
y1985 = twn[twn$Season == 1985,]
years = unique(twn$Season)
wTeamEntries = unique(y1985$Wteam)
uniqueTeamy = unique(newSeeds[newSeeds$Season==1985,])
getYears = function(allData,years,season)
{
tot = list()
for(index in 1:length(years))
{
maxNum = list()
yData = allData[allData$Season == years[index],]
teams = season[season$Season==years[index],]$Team
for(val in 1:length(yData))
{
maxNum[[val]]=c(yData$Lteam[val],yData$Daynum[val])
}
tot[index] = maxNum
}
return(tot)
}
results = getYears(twn,years,newSeeds)
getYears(twn,years,newSeeds)warnings()
warnings()
getYears = function(allData,years,season)
{
tot = list()
for(index in 1:length(years))
{
maxNum = list()
yData = allData[allData$Season == years[index],]
teams = season[season$Season==years[index],]$Team
for(val in 1:length(yData))
{
maxNum[[val]]=c(yData$Lteam[val],yData$Daynum[val])
print(maxNum)
}
tot[index] = maxNum
}
return(tot)
}
results = getYears(twn,years,newSeeds)
getYears = function(allData,years,season)
{
tot = list()
for(index in 1:length(years))
{
maxNum = list()
yData = allData[allData$Season == years[index],]
teams = season[season$Season==years[index],]$Team
for(val in 1:length(yData))
{
maxNum[[val]]=c(yData$Lteam[val],yData$Daynum[val])
print(length(maxNum))
}
tot[index] = maxNum
}
return(tot)
}
results = getYears(twn,years,newSeeds)
twn
yData = twn[twn$Season == years[1],]
yData
maxNum = list()
for(val in 1:length(yData))
{
maxNum[val]=c(yData$Lteam[val],yData$Daynum[val])
print(length(maxNum))
}
for(val in 1:length(yData))
{
maxNum[[val]]=c(yData$Lteam[val],yData$Daynum[val])
print(length(maxNum))
}
maxNum[[1]]
yData$Lteam
yData$Lteam[1]
